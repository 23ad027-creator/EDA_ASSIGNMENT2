"""
IMDB Movie Review Rating Prediction (Regression)
Exploratory Data Analysis and Visualization (U21ADP05)
Author: Kathir Kavin Kumar M
Roll No: 23AD027
Department: Artificial Intelligence & Data Science
Date of Submission: 20 October 2025

This script follows the notebook-style pipeline:
- Load dataset (expects 'IMDB_Dataset.csv' with columns: 'review' and 'rating')
- Basic EDA & visualizations (saved to PNG files)
- Text cleaning and TF-IDF feature extraction
- Train/Val/Test split (70/15/15)
- Build a Keras MLP regression model
- Train with EarlyStopping and save history plots
- Evaluate on test set and save predictions & residual plots
- Save trained model (HDF5) and TF-IDF vectorizer (joblib)
"""

import os
import re
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import joblib
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import warnings
warnings.filterwarnings('ignore')
sns.set(style='whitegrid')

# -------------------- Helper functions --------------------
def clean_text(text):
    if pd.isna(text):
        return ""
    text = str(text)
    text = text.lower()
    text = re.sub(r'<br\s*/?>', ' ', text)   # remove HTML line breaks
    text = re.sub(r'http\S+', ' ', text)     # remove urls
    text = re.sub(r'[^a-z0-9\s]', ' ', text) # remove punctuation
    text = re.sub(r'\s+', ' ', text).strip() # collapse whitespace
    return text

def ensure_dir(d):
    if not os.path.exists(d):
        os.makedirs(d)

# -------------------- Load dataset --------------------
def load_dataset(path='IMDB_Dataset.csv'):
    if os.path.exists(path):
        df = pd.read_csv(path)
        # try common column names
        if 'review' in df.columns and 'rating' in df.columns:
            return df[['review','rating']].copy()
        # try alternatives
        if 'text' in df.columns and 'rating' in df.columns:
            return df[['text','rating']].rename(columns={'text':'review'})
        if df.shape[1] >= 2:
            # assume first column is text, second is rating
            df = df.iloc[:, :2]
            df.columns = ['review','rating']
            return df
        raise ValueError("CSV does not have expected columns: 'review' and 'rating'")
    else:
        # small demo dataset
        data = [
            ("I loved the movie, it was fantastic and well acted.", 9),
            ("Terrible film. I wasted two hours. Bad plot.", 2),
            ("It was okay â€” some good parts but too long.", 6),
            ("Outstanding performance by lead actor. Very moving.", 8),
            ("Not my type. Too slow and boring.", 3),
            ("Great visuals and soundtrack. Enjoyed it.", 8),
            ("Mediocre storyline but good effects.", 5),
            ("Excellent script and direction. Highly recommended.", 9),
            ("Poor acting and weak dialogue.", 3),
            ("Average movie, watch at your own risk.", 5)
        ]
        return pd.DataFrame(data, columns=['review','rating'])

# -------------------- EDA --------------------
def basic_eda(df, out_dir='outputs'):
    ensure_dir(out_dir)
    print("Dataset shape:", df.shape)
    print("Missing values:\n", df.isna().sum())
    print("Rating stats:\n", df['rating'].describe())
    # Rating distribution
    plt.figure(figsize=(6,4))
    sns.histplot(df['rating'], bins=10, kde=True)
    plt.title('Rating Distribution')
    plt.xlabel('Rating')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir,'rating_distribution.png'))
    plt.close()
    # Review length distribution
    df['review_len'] = df['review'].astype(str).apply(lambda x: len(x.split()))
    plt.figure(figsize=(6,4))
    sns.histplot(df['review_len'], bins=30, kde=True)
    plt.title('Review Length Distribution (words)')
    plt.xlabel('Word Count')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir,'review_length_distribution.png'))
    plt.close()
    # Rating vs length scatter
    plt.figure(figsize=(6,4))
    sns.scatterplot(x='review_len', y='rating', data=df, alpha=0.6)
    plt.title('Review Length vs Rating')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir,'length_vs_rating.png'))
    plt.close()
    # Top words (simple)
    all_text = ' '.join(df['review'].astype(str).apply(clean_text).tolist())
    words = [w for w in all_text.split() if len(w)>2]
    from collections import Counter
    top = Counter(words).most_common(20)
    top_df = pd.DataFrame(top, columns=['word','count'])
    plt.figure(figsize=(8,4))
    sns.barplot(x='count', y='word', data=top_df)
    plt.title('Top 20 Words (cleaned)')
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir,'top_words.png'))
    plt.close()

# -------------------- Build model --------------------
def build_mlp(input_dim, hidden_layers=[128,64], dropout=0.2, lr=1e-3):
    model = keras.Sequential()
    model.add(layers.InputLayer(input_shape=(input_dim,)))
    for units in hidden_layers:
        model.add(layers.Dense(units, activation='relu'))
        model.add(layers.Dropout(dropout))
    model.add(layers.Dense(1, activation='linear'))
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),
                  loss='mse',
                  metrics=[keras.metrics.RootMeanSquaredError(name='rmse'),
                           keras.metrics.MeanAbsoluteError(name='mae')])
    return model

# -------------------- Training pipeline --------------------
def train_pipeline(df, out_dir='outputs', random_state=42):
    ensure_dir(out_dir)
    # Clean text
    df['clean_review'] = df['review'].apply(clean_text)
    # Features and target
    X_text = df['clean_review'].values
    y = df['rating'].astype(float).values
    # Train/Val/Test split 70/15/15
    X_train, X_temp, y_train, y_temp = train_test_split(X_text, y, test_size=0.30, random_state=random_state)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)
    print("Train/Val/Test sizes:", len(X_train), len(X_val), len(X_test))
    # TF-IDF Vectorizer (fit on train)
    tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1,2))
    X_train_tfidf = tfidf.fit_transform(X_train)
    X_val_tfidf = tfidf.transform(X_val)
    X_test_tfidf = tfidf.transform(X_test)
    # Optionally scale (MLP benefits from scaled inputs)
    scaler = StandardScaler(with_mean=False)  # with_mean=False because sparse input
    X_train_scaled = scaler.fit_transform(X_train_tfidf)
    X_val_scaled = scaler.transform(X_val_tfidf)
    X_test_scaled = scaler.transform(X_test_tfidf)
    # Build model
    model = build_mlp(input_dim=X_train_scaled.shape[1], hidden_layers=[128,64], dropout=0.2, lr=1e-3)
    model.summary()
    # Callbacks
    early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)
    # Train
    history = model.fit(X_train_scaled.toarray(), y_train, 
                        validation_data=(X_val_scaled.toarray(), y_val),
                        epochs=100, batch_size=32, callbacks=[early_stop, reduce_lr], verbose=2)
    # Save history plots
    plt.figure(figsize=(6,4))
    plt.plot(history.history['loss'], label='train_loss')
    plt.plot(history.history['val_loss'], label='val_loss')
    plt.xlabel('Epoch'); plt.ylabel('MSE Loss'); plt.title('Loss vs Epoch')
    plt.legend(); plt.tight_layout()
    plt.savefig(os.path.join(out_dir,'loss_vs_epoch.png')); plt.close()
    plt.figure(figsize=(6,4))
    if 'mae' in history.history:
        plt.plot(history.history.get('mae', []), label='train_mae')
        plt.plot(history.history.get('val_mae', []), label='val_mae')
        plt.xlabel('Epoch'); plt.ylabel('MAE'); plt.title('MAE vs Epoch')
        plt.legend(); plt.tight_layout()
        plt.savefig(os.path.join(out_dir,'mae_vs_epoch.png')); plt.close()
    # Evaluate on test set
    preds = model.predict(X_test_scaled.toarray()).flatten()
    mse = mean_squared_error(y_test, preds)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)
    print(f"Test MSE: {mse:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}")
    # Save predicted vs actual plot
    plt.figure(figsize=(6,6))
    sns.scatterplot(x=y_test, y=preds, alpha=0.6)
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
    plt.xlabel('Actual Rating'); plt.ylabel('Predicted Rating'); plt.title('Predicted vs Actual')
    plt.tight_layout(); plt.savefig(os.path.join(out_dir,'predicted_vs_actual.png')); plt.close()
    # Residuals
    residuals = y_test - preds
    plt.figure(figsize=(6,4))
    sns.histplot(residuals, kde=True)
    plt.title('Residual Distribution'); plt.tight_layout(); plt.savefig(os.path.join(out_dir,'residuals.png')); plt.close()
    # Save artifacts: model, tfidf, scaler, metrics
    model.save(os.path.join(out_dir,'imdb_mlp_regressor.h5'))
    joblib.dump(tfidf, os.path.join(out_dir,'tfidf_vectorizer.joblib'))
    joblib.dump(scaler, os.path.join(out_dir,'scaler.joblib'))
    metrics = {'mse': float(mse), 'rmse': float(rmse), 'mae': float(mae), 'r2': float(r2)}
    with open(os.path.join(out_dir,'metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    print("Artifacts saved to", out_dir)
    return metrics

# -------------------- Main --------------------
def main():
    print("Loading dataset...")
    df = load_dataset('IMDB_Dataset.csv')
    print(df.head(3))
    basic_eda(df, out_dir='outputs')
    metrics = train_pipeline(df, out_dir='outputs')
    print("Finished. Metrics:", metrics)

if __name__ == '__main__':
    main()
